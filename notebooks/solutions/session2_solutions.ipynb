{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark for astronomy: hands-on session 2\n",
    "\n",
    "### Context\n",
    "\n",
    "Welcome to the series of notebooks on Apache Spark! The main goal of this series is to get familiar with Apache Spark, and in particular its Python API called PySpark in a context of the astronomy. In this second notebook, we will learn on concrete examples how to interface and play with popular scientific libraries (Numpy, Pandas, ...).\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Interfacing popular Python scientific libraries with Apache Spark\n",
    "- Developing your own modules for Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this series of exercises, we will use the same dataset as in the first session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a Spark DataFrame\n",
    "df = spark.read.format(\"parquet\").load(\"../../data/clusters.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User defined functions and column creation\n",
    "\n",
    "Similarly to `map` and `mapPartitions`, you would like to define your own functions but this time to create new DataFrame columns. In python, the efficient way of doing this is via \"Pandas User Defined Functions\" (vectorized functions). \n",
    "\n",
    "**Exercise (£):** Use pandas UDF to compute the distance of each row to the center, and store the result in a new Dataframe column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+---+------------------+\n",
      "|                  x|                  y|                 z| id|          distance|\n",
      "+-------------------+-------------------+------------------+---+------------------+\n",
      "|-1.4076402686194887|  6.673344773733206| 8.208460943517498|  2|10.672104415540714|\n",
      "| 0.6498424376672443|  3.744291410605022|1.0917784706793445|  0|3.9539845207540685|\n",
      "| 1.3036201950328201|-2.0809475280266656| 4.704460741202294|  1|5.3067616389669645|\n",
      "|-1.3741641126376476|  4.791424573067701| 2.562770404033503|  0| 5.604807631993114|\n",
      "| 0.3454761504864363| -2.481008091382492|2.3088066072973583|  1|3.4066615432062313|\n",
      "+-------------------+-------------------+------------------+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(DoubleType(), PandasUDFType.SCALAR)\n",
    "def compute_distance_to_center(x, y, z):\n",
    "    \"\"\" Compute the distance to the center (0, 0, 0)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y, z: double\n",
    "        row coordinates\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    series: pandas Series\n",
    "        Series containing distance to the center for each row\n",
    "    \"\"\"\n",
    "    r_square = x*x + y*y + z*z\n",
    "    return pd.Series(np.sqrt(r_square))\n",
    "\n",
    "df.withColumn(\n",
    "    \"distance\", \n",
    "    compute_distance_to_center(\n",
    "        col(\"x\"),\n",
    "        col(\"y\"),\n",
    "        col(\"z\")\n",
    "    )\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (£££):** As in session 1, find the barycentre of each clusters in the dataset but this time using aggregation and user defined function (hint: look for `PandasUDFType.GROUPED_MAP`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+---+\n",
      "|                  x|                  y|                 z| id|\n",
      "+-------------------+-------------------+------------------+---+\n",
      "| 0.9084311322436593|-1.5335608883132903| 2.926201255363395|  1|\n",
      "|-1.2364938227997018| 7.7837163227456205| 9.292937669035544|  2|\n",
      "|  1.001314312562809|  4.250879907797302|2.0216900721305446|  0|\n",
      "+-------------------+-------------------+------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf(df.schema, PandasUDFType.GROUPED_MAP)\n",
    "def compute_barycentre(pdf: pd.DataFrame):\n",
    "    \"\"\" Compute the barycentre of a partition\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf : pandas DataFrame\n",
    "        pandas DataFrame containing partition data\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    Pandas DataFrame with barycentre coordinates.\n",
    "    \"\"\"\n",
    "    # We can use Pandas method directly\n",
    "    mean = pdf.mean()\n",
    "    # This is just to reconstruct a Pandas DataFrame with the result\n",
    "    # i: colname, j: mean value\n",
    "    out = {i:[j] for i, j in zip(mean.keys(), mean.values)}\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "df.groupBy(\"id\").apply(compute_barycentre).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster coordinates:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.00131431, 4.25087991, 2.02169007]),\n",
       " array([ 0.90843113, -1.53356089,  2.92620126]),\n",
       " array([-1.23649382,  7.78371632,  9.29293767]),\n",
       " [None, None, None]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def yield_barycentre(part):\n",
    "    \"\"\" Yield the number of rows in the partition\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    part : Iterator\n",
    "        Iterator containing partition data\n",
    "        \n",
    "    Yield\n",
    "    ----------\n",
    "    length: integer\n",
    "        number of rows inside the partition\n",
    "    \"\"\"\n",
    "    try:\n",
    "        partition_data = [*part]\n",
    "        x, y, z, _ = np.transpose(partition_data)\n",
    "        yield np.mean([x, y, z], axis=1)\n",
    "    except ValueError as e:\n",
    "        # Empty partition\n",
    "        yield [None, None, None]\n",
    "\n",
    "# Let's repartition our DataFrame according to \"id\"\n",
    "df_repart = df.orderBy(\"id\")\n",
    "\n",
    "# mapPartitions is a RDD method(not available for DataFrame in pyspark)\n",
    "print(\"Cluster coordinates:\")\n",
    "df_repart.rdd.mapPartitions(yield_barycentre).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
