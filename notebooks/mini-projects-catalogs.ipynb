{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project 1: Galaxy Clusters & Velocity\n",
    "\n",
    "**Context**\n",
    "\n",
    "This notebook illustrates the basics of accessing a galaxy catalog from N-body simulations through Apache Spark as well as how to select useful samples of data to study galaxy clusters. Data would be representative (although we will play with a very small set) to LSST simulations data. For more about Apache Spark in the context of astronomy, connect to [AstroLab Software](https://astrolabsoftware.github.io/)!\n",
    "\n",
    "**Learning objectives**\n",
    "\n",
    "After going through this notebook, you should be able to:\n",
    "\n",
    "- Load and efficiently access a galaxy catalog with Apache Spark\n",
    "- Apply cuts to the catalog using Spark SQL functionalities\n",
    "- Have several example of quality cuts and validation procedures \n",
    "- Derive scientific results on galaxy clusters\n",
    "- Distribute the computation and the plotting routine to be faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what you would probably need\n",
    "from typing import Iterator, Generator, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from pyspark.sql.functions import pandas_udf , PandasUDFType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data with Spark\n",
    "\n",
    "Let's load the `data/catalogs` data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data\n",
    "fn = \"../data/catalogs\"\n",
    "\n",
    "# Load the data - this a lazy operation, no data movement yet!\n",
    "df = spark.read.format(\"parquet\").load(fn)\n",
    "\n",
    "# Let's inspect the schema\n",
    "df.printSchema()\n",
    "\n",
    "# Number of objects in the catalog\n",
    "print(\"Number of rows: {}\".format(df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look a some mass values. Apache Spark provides filter mechanisms, which you can use to speed up data access if you only need a certain chunks of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reject synthetic halos (negative halo_id) that are added to host the ultra-faint galaxies\n",
    "cols = [\"halo_mass\", \"stellar_mass\", \"blackHoleMass\", \"halo_id\"]\n",
    "df.filter(\"halo_id > 0\").select(cols).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `halo_mass` is duplicated for all the members of the same halo. \n",
    "We can also easily look at statistics about individual columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the stellar_mass and halo_mass distributions\n",
    "df.select([\"stellar_mass\", \"halo_mass\", \"redshift\"]).describe().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Halo mass distribution\n",
    "\n",
    "To start this journey, let's look at the distribution of halo masses in the catalog.\n",
    "\n",
    "**Exercise (£):** Create 3 DataFrames from `df` with different populations:\n",
    "- Full data set (all redshift)\n",
    "- low-z (0.0 < z < 0.2) clusters\n",
    "- high-z (2.5 < z < 3.1) clusters\n",
    "\n",
    "In the three cases, you will select only central galaxies, and clusters with positive `halo_id` (i.e. we reject the synthetic halos that are added to host the ultra-faint galaxies). (hint: to speed up the computation, do not forget cache capabilities!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of clusters is rather high, we will capitalize on the fact that we are doing computation in parallel.\n",
    "The way to be faster is to distribute the computation which leads to the data to be plotted. Histograms are particularly easy to distribute:\n",
    "- Load the data set (distributed accross machines)\n",
    "- Apply filters on lines and select columns (order does not matter as Spark will choose the optimal way). Partitions will be processed in parallel. If you have more partitions than workers (typically CPU), there will be a partition queue.\n",
    "- With the remaining data in each partition, build an histogram per partition.\n",
    "- Reduce to the driver all partition histograms by summing them up. You have the final histogram!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (£££):** Write such a method to be applied on each Spark partition to compute histograms in parallel (each would contain only a fraction of the data). Hint: `mapPartitions` and `numpy` could be your friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A weird feature!\n",
    "\n",
    "We selected the clusters based on their `halo_id`, and to avoid double counting we took only entries corresponding to central galaxies (all galaxies for a given `halo_id` have the same `halo_mass`).\n",
    "\n",
    "**Exercise (£):** Inspect the `halo_mass` distribution values. What do you observe? Hint: did you see the attack of the clones? (and also capitalise on the histogram method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Galaxy clusters and velocity in DC2\n",
    "\n",
    "### A few individual galaxy clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now focus on some selected galaxy clusters. \n",
    "\n",
    "**Exercise (£):** Extract 5 rich clusters ($n_{gal} > 20$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean velocity as a function of redshift\n",
    "\n",
    "To end this journey, let's have a look at the mean velocity distribution as a function of redshift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redshift range\n",
    "redshift_start = 0.0\n",
    "redshift_stop = 3.0\n",
    "redshift_step = 0.5\n",
    "redshift_window = 0.1\n",
    "values = np.arange(redshift_start, redshift_stop, redshift_step)\n",
    "\n",
    "# start at 0.2 because stat is poor at very low redshift\n",
    "values[0] = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (££):** Make histograms of the mean 3D velocity of haloes, at different redshift ranges. Hint: look at `groupBy` and `agg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The velocity dispersion–halo mass relation\n",
    "\n",
    "**Exercise (££££):** compute the velocity dispersion–halo mass relation. The principle is similar to the M-$\\sigma$ relation for stars around black holes. The idea is to highlight the fact that gravitational interaction/friction between galaxies has an effect for the cluster galaxy evolution.\n",
    "\n",
    "Hints:\n",
    "- Use pandas UDF to compute the 3D velocity norm.\n",
    "- Use `stddev_pop` combined to `groupBy`.\n",
    "- Use DataFrame `join` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To go further: fitting the data\n",
    "\n",
    "Following e.g. [1602.00611](https://arxiv.org/abs/1602.00611) (see section 4.1), we can model the mean relation between velocity dispersion and halo mass at a given redshift using a simple power-law of the form:\n",
    "\n",
    "$$\\sigma_v(\\rm{M}_h) = \\alpha \\Big( \\dfrac{\\rm{M}_h}{10^{14}\\rm{M}_\\odot} \\Big)^{\\beta}$$\n",
    "\n",
    "and the dependency in redshift is given by\n",
    "\n",
    "$$\\sigma_v(z) = \\sigma_v(0)\\sqrt{1 + z}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
